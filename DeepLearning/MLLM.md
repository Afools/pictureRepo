# 多模态大模型

## Bert

Bert 是一个基于 Transformer 的预训练语言模型，主要用于自然语言处理任务。它通过双向编码器表示（Bidirectional Encoder Representations from Transformers）来捕捉上下文信息。Bert 的预训练阶段包括两个任务：Masked Language Model（MLM）和 Next Sentence Prediction（NSP）。

其结构主要由多层 Transformer 编码器堆叠而成，每个编码器包含自注意力机制和前馈神经网络。Bert 的输入是一个句子对，输出是一个向量表示，可以用于分类、问答等任务。Bert 的预训练和微调策略使其在多个自然语言处理任务上取得了显著的性能提升。

## Blip

Blip（Bootstrapping Language-Image Pre-training）是一个多模态预训练模型，旨在通过联合学习图像和文本信息来提高视觉和语言任务的性能。Blip 采用了自监督学习的方法，通过生成图像描述和图像填充任务来预训练模型。其结构主要由视觉编码器和语言编码器组成，视觉编码器用于提取图像特征，语言编码器用于处理文本信息。Blip 在多个视觉-语言任务上表现出色，如图像标注、视觉问答等。

其结构主要由视觉编码器（通常是 CNN 或 ViT）和语言编码器（通常是 Transformer）组成。Blip 的预训练任务包括图像描述生成和图像填充，通过这些任务，模型能够学习到图像和文本之间的深层次关系。Blip 在多个视觉-语言任务上表现出色，如图像标注、视觉问答等。

## Blip-2

Blip-2 引入了冻结的 LLM。核心是 Q-Former,主要创新点在于使用 Query 来引导图像的特征提取过程，每一个可学习的 Query 对应一个视觉特征输出，在计算相似度时使用相似度最高的 Query 输出。

## ViT

ViT（Vision Transformer）是一个基于 Transformer 的视觉模型，旨在通过自注意力机制来处理图像数据。ViT 将图像划分为多个小块（patch），然后将这些小块展平并输入到 Transformer 中进行处理。ViT 的主要优点是能够捕捉长距离依赖关系，并且在大规模数据集上表现出色。ViT 在多个计算机视觉任务上取得了显著的性能提升，如图像分类、目标检测等。

ViT 的结构主要由多个 Transformer 编码器堆叠而成，每个编码器包含自注意力机制和前馈神经网络。ViT 的输入是一个图像块序列，输出是一个图像表示向量，可以用于分类、检测等任务。

## Clip

CLip（Contrastive Language-Image Pre-training）是一个多模态预训练模型，旨在通过对比学习来联合学习图像和文本信息。CLIP 采用了自监督学习的方法，通过对比图像和文本描述来预训练模型。其结构主要由视觉编码器和语言编码器组成，视觉编码器用于提取图像特征，语言编码器用于处理文本信息。CLIP 在多个视觉-语言任务上表现出色，如图像标注、视觉问答等。

CLip 如何做到图像和文本信息对齐：
CLIP 通过对比学习的方法，将图像和文本描述映射到同一特征空间中。具体来说，CLIP 首先将图像和文本分别编码为向量，然后计算它们之间的相似度。通过最大化正确图像-文本对的相似度，同时最小化错误对的相似度，CLIP 能够学习到图像和文本之间的深层次关系。

## LLava

LLaVA 使用一个简单的 MLP 层将图像特征映射到文本特征空间。其训练的两个步骤是：

1. 固定的视觉编码器（如 CLIP）和 LLM，预训练 MLP 实现图像和文本的对齐。
2. 通过视觉-语言对齐的提示微调 LLM。

**LLaVA 1.5**
LLaVA 1.5 是 LLaVA 的改进版本，主要在以下几个方面进行了优化：

1. 将一层 MLP 替换为两层 MLP，增加了模型的表达能力。
2. 使用高分辨率的图像进行分割后直接编码，而不是先进行下采样。
3. 对于回复的长短句进行明确的标注，增加了模型对长文本的处理能力。
