# Essay Note: "An Overview of Federated Deep Learning Privacy Attacks and Defensive Strategies"

[原文链接](https://arxiv.org/pdf/2004.04676.pdf)

## Introduction

联邦学习（Federated learning, FL）的构想来起源于“中央服务器不需要真实的确切数据”这样一个想法。一个抽象的机器学习模型代替原始数据传递给服务器。这些模型在本地先进行训练再传给服务器，服务器合并这些模型得到一个聚合模型并传回客户端。  
联邦学习能够有效的防止数据的外部威胁但不能防止恶意用户等内部威胁。  
此文章给出一个攻击联邦学习策略的综述和基本分类法，以及防御法的综述。

## Threat Model

![Fig2](https://raw.githubusercontent.com/Afools/pictureRepo/main/FDL_Att&Def_Strategies_fig2.png)

### Attack Surface（AS）

攻击者的主要目的：

1. 从客户端得到私人数据
2. 干扰模型运作（引入后门，诱发错误甚至破坏模型）

![Fig1](https://raw.githubusercontent.com/Afools/pictureRepo/main/FDL_Att&Def_Strategies_fig1.png)

FL 的 AS 根据情况分为客户端威胁和服务器威胁,每个环节的 AS 数量如 Fig1 所示。  
服务器威胁的 attack surface 由以下这些可利用点组成：

1. 有能力修改发送给客户端的模型或能控制有发送模型能力的客户端,甚至能向客户端发送模型
2. 能区分来自客户端的合并前的经过训练的模型
3. 服务器能够了解聚合后的模型信息。

客户端威胁的 attack surface 由以下可利用点组成：

1. 客户端能够访问聚合后的模型
2. 能够操纵客户端原数据和控制训练流程
3. 能够操作梯度迭代
4. 能够影响此模型在聚合时的影响力

### Black-box or white-box attacks

白盒攻击指攻击者了解系统除了私人数据之外的全部信息。  
 黑盒攻击指攻击者只能使用系统而不了解系统内部机制。  
 联邦学习算法中客户端假定拥有模型的全部信息，因此容易受到白盒攻击。服务端也是如此，但最近研究的新模型使服务器只能了解模型概念而无法了解更多模型信息。

### Active or passive attacks

主动攻击通过修改模型参数，通信或其他系统属性达到恶意目的。这种攻击通常是可以被检测的。  
被动攻击不用修改任何系统参数，很难留下痕迹，很难检测，是一种更加危险的攻击方法。

### Attacker goal

1. sample reconstruction 重建样本
2. Information inference 推测隐私信息：得到隐私数据，包括分类代表，样本，标签，数据特征
3. Model corruption 破坏模型：根据攻击者的利益改变模型
4. Runtime misclassification 运行时混淆：模型运行时出现错误结果

> 重建样本 sample reconstruction 和推测隐私信息 Information inference 有类似的目标，如何区分？

## Attack methods

### Attacks targeting reconstruction

这里介绍了三种针对数据重建的攻击方法。

1. Loss-function/ReLu exploitation  
   通过对 ReLu 算法的输入-损失关系进行分析，还原输入数据。此方法只能用于使用线性模型和 ReLu 函数的深度神经网络，并且在对抗噪声中表现较弱，这可能由于此方法使用的严格数学机制。
2. First dense layer attack  
   通过神经元累计的梯度与前一层神经元激活状态的联系来还原数据。此方法在一个用单个样本训练的模型场景中成功重建了这个样本。
3. DLG/iDLG  
   Deep Leakage From Gradients (DLG) 通过一种最优化算法迭代还原样本，达到与客户端模型相似的结果。这个算法的一种变种对较大的数据集也有效。

### Attacks targeting inference

1. Model inversion attacks (MIA)
   适用于线性模型,易于防备，对大规模输入在计算量上不可行。
2. mGAN-AI
   multitask GAN for Auxiliary Identification (mGAN-AI) 辅助识别的多任务生成对抗网络。通过所有来自客户端的数据和一个辅助数据集来训练一个 GAN。GAN 生成一系列可以得到相同更新的假的图片，一个鉴别网络来识别这些图片的真假，类别，以及归属于哪一个客户端。
   此方法适用于客户端的大部分数据属于同一类的情况并且需要一个有效的辅助数据集。这种方法利用 GAN，因此只适用于<span style="color:red">_可综合的数据_</span>。
   > 可综合的数据如何定义  
   > "可综合的数据"或者说 "data that can be synthesized" 指的是可以通过某种方式生成的数据。在这种情况下，它特指可以通过生成对抗网络（GAN）生成的数据。GAN 是一种深度学习模型，它由两部分组成：一个生成器和一个判别器。生成器的任务是生成尽可能真实的假数据，而判别器的任务是区分真实数据和假数据。  
   > GAN 只能应用于可综合的数据，是因为 GAN 的工作方式决定的。GAN 通过学习真实数据的分布来生成新的、与真实数据相似的假数据。如果数据的分布复杂或者难以学习（例如，高维数据、结构化数据等），那么 GAN 可能无法生成高质量的假数据。此外，如果数据的分布本身就是离散的或者无法通过连续的潜在空间表示（例如，文本数据），那么 GAN 也可能无法应用。  
   > 因此，"可综合的数据"通常指的是那些可以通过学习其分布来生成新数据的数据，例如图像、音频等。
3. GAN
   这种攻击方法通过对在模型中下毒来获得私密数据。一个简单应用场景如下，先通过共享模型获得目标客户端的独有标签，通过 GAN 生成这种标签的图像并标记为该标签。生成的图像逐渐接近目标客户端原数据，受害者的模型梯度会变得更加陡峭。  
   这种方法假定目标客户端和服务器享有一个共享标签和一个排他标签，但是在样本数很大的情况下这是不现实的。另外，因为联邦中选择客户端的随机性和平均权重时客户端影响力的衰退，在大量客户端中分辨出目标客户端是不具备可行性的。

## Attacks targeting misclassification

本文中只讨论对抗性样例攻击(adversarial example attack)，攻击者通过制造能够使模型在运行时出现错误分类的样本为目标，扰乱模型学习。联邦学习的分布式学习特点使其在这种攻击面前几乎不设防，因此联邦学习非常容易受到误分类攻击影响，比如在用于设备认证（如人脸识别）的模型中这种攻击会非常有效。

## Attacks targeting model corruption

本文只讨论模型下毒攻击(model poisoning attacks)。每一个客户端都对最终模型有直接的影响力，因此毒模型对聚合后的模型影响力会非常分散。这种攻击可以用在对模型引入后门的攻击中。  
Bagdasaryan 提出的攻击方法为，通过声名客户端拥有大量样本来提高此客户端对聚合模型的影响力，这种方法可以有效的在模型中留下后门，文中举出两个应用场景：1. 在语言生成模型中提高某品牌的出现几率。2. 在图像审核中将目标政治人物归类为不合适。
Bhagoji 等人的相关工作，他们进一步证明了这种攻击方法的可行性，并展示了如何以隐秘的方式使用这种攻击方法。此外，他们还展示了如何规避拜占庭健壮的聚合机制(byzantine-robust aggregation mechanism)。这些类型的聚合算法特别设计用来减少单个客户端的影响。这种机制通过一些特定的算法（例如，中位数聚合、Krum 等）来减少单个客户端的影响，从而提高模型的健壮性。

# Defensive Measures

这一章介绍一些在基础 FL 算法上增强保密性的策略。
![Fig3](https://raw.githubusercontent.com/Afools/pictureRepo/main/FDL_Att&Def_Strategies_fig3.png)
图三中显示对图一中的攻击面的保护措施的有效性。
![Fig4](https://raw.githubusercontent.com/Afools/pictureRepo/main/FDL_Att&Def_Strategies_fig4.png)
图四中列举防御措施及其对前文提到的攻击方式的防御有效性。其中~符号表示有限的效果，攻击者可能需要采取额外措施来达到目的或无法确保攻击结果的有效性（比如重建数据含有大量噪声）。\*符号表示这种方法受使用场景影响很大，比如差分隐私方法可能导致联合训练的模型无法收敛。

## Gradient subset

梯度子集方法通过只传输最重要的梯度，可以大大减少通信的数据量，从而提高通信效率。同时，这种方法也可以提高模型的性能，特别是在数据分布不均匀（非 IID）的情况下。这种防御措施的主要思想是，通过减少服务器从每个客户端获取的信息，可以降低攻击者通过分析模型更新来获取敏感信息的可能性。然而，这种方法并不完全有效，最近的研究表明攻击者可能仍然能够从有限的信息中推断出可靠的信息。

## Gradient compression

有损压缩能使服务器只获得客户端的不完整信息，同时能节省通信成本。另外，有文献提出服务器训练一种加密器，将加密部份分配给客户端，解密部份保留在服务端，加密器同时扮演压缩工具，对客户端的梯度结果进行压缩。这种方法在增加通信效率的同时，会在加密-解密过程中产生数据损失。这种方法的防御性能还有待观察。

## Dropout

Dropout 是一种被广泛应用于机器学习的策略，主要用途是防止过拟合。这种方法引入了随机性梯度更新，能一定程度上防止出现决定性梯度，减少攻击面。但是因为 Dropout 的特征泛化性质，这种策略在面对提取泛化数据的攻击时可能起到反作用。

## Difference privacy (DP)

拆分隐私是一种通过在变量中引入噪声确保隐私的机制。这种机制严密可证明，是最有效的隐私标准之一。目前此方法在 FL 中应用较为困难，它更适用于每个客户端都能访问相当大数据集的情况。但联邦学习中客户端彼此的数据集规模可能有很大差异。Bonawitz[53]提出了一种在 LSTM 语言模型服务端的 DP 算法，防止客户端之间的攻击行为。Geyer 等人扩展了这项工作，使其运行在图像识别网络。Triastcyn 等人演示了贝叶斯 DP 在拥有相似数据分布客户端的 FL 网络中起到了加速收敛的作用。贝叶斯 DP 策略在这种情况下引入较低的噪声量同时保持相同甚至更严格的隐私限制。

## Secure multiparty computation (SMC)

安全多方计算方案是允许两个或多个参与者在不向另一方泄露任何数据的情况下共同计算其集体数据的函数的算法，这种方式可以保护客户端不受服务器影响。通常 SMC 的参与方只有两方，所以在联邦学习中，通常用两个不可信的服务器来连接部份或全部的参与客户端。  
除了双方计算，[60]-[63]三方计算也被提出。这种方式依赖于至少半可信的参与者或者一个可信的主体。[60]Mohassel 等人提出三个使用 SMC 的服务器来聚合机器学习模型，这种方式还能发展成由服务器构成的分层结构。  
对于联邦学习，一个关键的弱点是服务器可以准确地知道特定的客户端梯度更新，从而可能推断出有关目标受害者客户端的信息。Bonawitz 等人[64]提出了几种协议的组合，通过模糊来自服务器的聚合来确保实际联邦设置中的安全性。
