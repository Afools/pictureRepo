# Machine Learning 技术学习笔记

## Classifier

### Gradient - Boosting Decision Tree 梯度提升决策树

#### 基本概念

在 GBDT 中，引入了梯度学习的方法对模型进行逐步迭代优化。对于分类任务，我们计算负梯度作为学习目标，通常为：
$$r_i=y_i-\frac{1}{1+e^{-F_{m-1}(x_i)}}$$
其中$F_{m-1}(x_i)$是第 m-1 次迭代的预测值。

#### 算法步骤

1. 初始化模型\
   在分类任务中我们通常使用对数几率(log odd)作为初始预测值，如$log(P)$，P 可以为某一分类在训练样本的出现几率。我们也可以简单的使用 0 作为初始预测值
2. 计算梯度\
   我们使用上述公式对训练数据的$y_i$和$x_i$进行梯度计算，得到训练目标$r_i$
3. 训练决策树\
   以$r_i$为目标，$x_i$为训练数据训练决策树$T_{m}$。
4. 迭代结果\
   将决策树的预测结果合并入最终预测中：$F_{m}=F_{m-1}+r*T_m(x)$ 其中$r$是学习率
5. 重复 2-4 步骤直到达到预设迭代数

### LightGBM Classifier

LightGBM Classifier 是基于 LightGBM（Light Gradient - Boosting Machine）框架的分类器，LightGBM 是微软开发的一个快速、高效的梯度提升框架，下面从基本概念、特点、工作原理、使用步骤、优缺点几个方面详细介绍 LightGBM Classifier。

#### 基本概念

LightGBM Classifier 是用于解决分类问题的机器学习模型，它利用梯度提升决策树（GBDT）算法，通过迭代地训练一系列弱分类器（决策树），并将它们组合成一个强分类器，从而对样本进行分类预测。

#### 特点

- 高效性
  训练速度快：采用了[直方图算法](#直方图算法)等优化技术，能够快速地对特征进行离散化和直方图统计，减少了计算量和内存占用，从而显著提高训练速度。
  内存占用少：直方图算法可以在内存中高效地存储和处理数据，相比于传统的 GBDT 算法，内存使用量大幅降低。
- 可扩展性
  支持大规模数据：能够处理大规模的数据集，对于高维数据和大量样本的分类问题表现出色。
  分布式计算：支持分布式训练，可以在多个节点上并行计算，进一步提高训练效率。
- 准确性
  高精度预测：通过梯度提升的方式不断优化模型，能够捕捉数据中的复杂模式和非线性关系，从而实现较高的分类准确率。
  支持多种数据类型：可以处理数值型、类别型等多种类型的特征，并且能够自动处理缺失值。

#### 工作原理

LightGBM Classifier 的核心是梯度提升决策树算法，其工作流程如下：

1. 初始化模型：初始时，模型的预测值通常设为一个常数（如所有样本标签的均值）。
2. 计算残差：对于每个样本，计算其真实标签与当前模型预测值之间的残差。
3. 训练弱分类器：使用当前的残差作为目标值，训练一棵新的决策树。在训练过程中，LightGBM 采用了直方图算法来选择最优的特征和分裂点，以快速构建决策树。
4. 更新模型：将新训练的决策树的预测结果加入到当前模型中，更新模型的预测值。
5. 重复步骤 2 - 4：不断迭代，直到达到预设的迭代次数或满足停止条件为止。
6. 最终预测：在预测阶段，将所有决策树的预测结果进行累加，得到最终的分类预测结果。

## 重要数学概念

### 直方图算法

#### 基本概念

在传统的梯度提升决策树（GBDT）算法中，为了找到每个特征上的最优分裂点，需要对所有样本的特征值进行排序，这在处理大规模数据时会消耗大量的计算资源和时间。而直方图算法通过将连续的特征值离散化到有限个区间（即直方图的 bins）中，将排序操作转化为简单的统计操作，从而显著减少了计算量。

#### 算法步骤

1. 特征离散化

- 对于每个特征，统计其所有样本的取值范围。
- 将该特征的取值范围划分为若干个不重叠的区间（bins），例如将取值范围 [0, 100] 划分为 10 个区间：[0, 10), [10, 20), …, [90, 100]。
- 遍历所有样本，将每个样本的特征值映射到对应的区间中，记录每个区间内样本的统计信息，如样本数量、梯度和等。

2. 构建直方图

- 根据特征离散化的结果，为每个特征构建一个直方图。直方图的每个 bin 对应一个区间，其值为该区间内样本的统计信息。
- 例如，对于一个包含 10 个 bin 的直方图，每个 bin 记录了落在该区间内样本的梯度和，这些统计信息将用于后续的分裂点查找。

3. 查找最优分裂点

- 对于每个特征的直方图，遍历所有可能的分裂点（即 bin 之间的边界）。
- 计算每个分裂点对应的[增益（如信息增益、基尼增益等）](#决策树模型的分裂增益)。
- 选择增益最大的分裂点作为该特征的最优分裂点。

4. 进行分裂

- 根据选择的最优分裂点，将数据集划分为左右两个子节点。
- 对于左右子节点，重复上述步骤 1 - 4，继续构建决策树的下一层。

#### 优点

1. 计算效率高\
   直方图算法将连续特征离散化后，只需要对有限个 bins 进行统计和比较，避免了对所有样本的特征值进行排序，大大减少了计算量。
2. 内存占用少\
   直方图只需要存储每个 bin 的统计信息，而不需要存储所有样本的原始特征值，因此可以显著减少内存使用。
   特别是在处理高维数据时，内存占用的优势更加明显。
3. 鲁棒性强\
   离散化过程对异常值有一定的鲁棒性，因为异常值会被映射到相应的 bin 中，不会像连续特征那样对分裂点的选择产生过大的影响。

#### 局限性

1. 信息损失\
   特征离散化过程会导致一定的信息损失，因为连续的特征值被映射到有限个区间中，可能会丢失一些细节信息。
   这种信息损失在某些情况下可能会影响模型的性能，特别是当特征的取值分布比较复杂时。
2. 分裂点选择受限\
   直方图算法只能在 bin 的边界上选择分裂点，而不能选择任意的连续值作为分裂点，这可能会导致无法找到全局最优的分裂点。
   不过，通过合理设置 bin 的数量，可以在一定程度上缓解这个问题。

### 决策树模型的分裂增益

在决策树模型中，我们通过分裂增益来判断生成的子节点对预测结果的提升程度，常用的分裂增益指标有信息增益，基尼系数等
决策树模型的分裂增益指标用于评估每个可能的分裂点的质量，从而选择最优的分裂点。不同的分裂增益指标适用于不同的任务（如分类、回归）和不同的损失函数。以下是常见的分裂增益指标及其详细介绍：

---

#### **1. 信息增益（Information Gain）**

信息增益是基于信息熵的指标，主要用于分类任务。它衡量的是分裂前后数据集的纯度提升。

##### **公式**

$$
\text{Information Gain} = H(Y) - \sum_{i=1}^k \frac{|D_i|}{|D|} H(Y_i)
$$

其中：

- $H(Y)$ 是分裂前数据集 $D$ 的熵。
- $H(Y_i)$ 是分裂后第 $i$ 个子集 $D_i$ 的熵。
- $k$ 是分裂后的子集数。
- $|D|$ 和 $|D_i|$ 分别是数据集和子集的大小。

##### **熵的计算**

$$
H(Y) = -\sum_{j=1}^C p_j \log_2(p_j)
$$

其中：

- $C$ 是类别数。
- $p_j$ 是类别 $j$ 在数据集中的比例。

##### **特点**

- 信息增益倾向于选择取值较多的特征（可能导致过拟合）。
- 通常与 **信息增益比（Gain Ratio）** 结合使用，以解决这一问题。

---

#### **2. 信息增益比（Gain Ratio）**

信息增益比是信息增益的改进版本，通过引入分裂信息（Split Information）来惩罚取值较多的特征。

##### **公式**

$$
\text{Gain Ratio} = \frac{\text{Information Gain}}{\text{Split Information}}
$$

其中：

- **Split Information** 衡量的是特征的分裂信息量：
  $$
  \text{Split Information} = -\sum_{i=1}^k \frac{|D_i|}{|D|} \log_2\left(\frac{|D_i|}{|D|}\right)
  $$

##### **特点**

- 信息增益比可以缓解信息增益对取值较多特征的偏好。
- 适用于分类任务。

---

#### **3. 基尼指数（Gini Index）**

基尼指数是另一种衡量数据集纯度的指标，主要用于分类任务。它表示从数据集中随机抽取两个样本，其类别不一致的概率。

##### **公式**

$$
\text{Gini Index} = 1 - \sum_{j=1}^C p_j^2
$$

其中：

- $p_j$ 是类别 $j$ 在数据集中的比例。

##### **分裂增益**

分裂后的基尼指数增益为：

$$
\text{Gini Gain} = \text{Gini}(D) - \sum_{i=1}^k \frac{|D_i|}{|D|} \text{Gini}(D_i)
$$

##### **特点**

- 基尼指数的计算比信息熵更快，因为它不需要对数运算。
- 适用于分类任务，是 CART 决策树的默认分裂指标。

---

#### **4. 方差减少（Variance Reduction）**

方差减少是用于回归任务的指标，衡量的是分裂前后目标值的方差变化。

##### **公式**

$$
\text{Variance Reduction} = \text{Var}(Y) - \sum_{i=1}^k \frac{|D_i|}{|D|} \text{Var}(Y_i)
$$

其中：

- $\text{Var}(Y)$ 是分裂前数据集 $D$ 的目标值方差。
- $\text{Var}(Y_i)$ 是分裂后第 $i$ 个子集 $D_i$ 的目标值方差。

##### **方差的计算**

$$
\text{Var}(Y) = \frac{1}{|D|} \sum_{i=1}^{|D|} (y_i - \bar{y})^2
$$

其中：

- $\bar{y}$ 是目标值的均值。

##### **特点**

- 适用于回归任务。
- 目标是使分裂后的子集目标值尽可能接近其均值。

---

#### **5. 平方误差损失（MSE）增益**

平方误差损失（Mean Squared Error, MSE）增益是另一种用于回归任务的指标，衡量的是分裂前后目标值的平方误差变化。

##### **公式**

$$
\text{MSE Gain} = \text{MSE}(D) - \sum_{i=1}^k \frac{|D_i|}{|D|} \text{MSE}(D_i)
$$

其中：

- $\text{MSE}(D)$ 是分裂前数据集 $D$ 的平方误差。
- $\text{MSE}(D_i)$ 是分裂后第 $i$ 个子集 $D_i$ 的平方误差。

##### **MSE 的计算**

$$
\text{MSE}(D) = \frac{1}{|D|} \sum_{i=1}^{|D|} (y_i - \bar{y})^2
$$

##### **特点**

- 适用于回归任务。
- 目标是使分裂后的子集目标值尽可能接近其均值。

---

#### **6. 梯度提升树（GBDT）的分裂增益**

在梯度提升树（如 XGBoost、LightGBM）中，分裂增益基于损失函数的一阶导数（梯度）和二阶导数（Hessian）。

##### **公式**

$$
\text{Gain} = \frac{(\sum g_L)^2}{\sum h_L + \lambda} + \frac{(\sum g_R)^2}{\sum h_R + \lambda} - \frac{(\sum g)^2}{\sum h + \lambda}
$$

其中：

- $g$ 是梯度，$h$ 是 Hessian。
- $L$ 和 $R$ 分别表示分裂后的左子树和右子树。
- $\lambda$ 是正则化参数。

##### **特点**

- 适用于分类和回归任务。
- 通过梯度和 Hessian 可以更精确地衡量分裂的质量。

---

#### **总结**

| 指标名称         | 适用任务  | 特点                                       |
| ---------------- | --------- | ------------------------------------------ |
| 信息增益         | 分类      | 基于信息熵，倾向于选择取值较多的特征。     |
| 信息增益比       | 分类      | 改进的信息增益，缓解对取值较多特征的偏好。 |
| 基尼指数         | 分类      | 计算速度快，适用于分类任务。               |
| 方差减少         | 回归      | 基于目标值的方差，适用于回归任务。         |
| 平方误差损失增益 | 回归      | 基于目标值的平方误差，适用于回归任务。     |
| GBDT 分裂增益    | 分类/回归 | 基于梯度和 Hessian，适用于梯度提升树模型。 |

根据任务类型和模型需求，选择合适的分裂增益指标可以显著提升决策树的性能。

## 实践部份
